{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train-ner",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNkoW+gODmMvB8mdz4sijvf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirjampaales/cool-ml-project/blob/main/named_entity_recognition/train_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading data \n",
        "This can be skipped if using the notebook withing the .git repository as the repositories below are included there as submodules:"
      ],
      "metadata": {
        "id": "3pkeTfcN3xWq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it8NGdCn2_l4",
        "outputId": "f54c33ed-8f59-4e4c-e8d7-ca5e751aa2ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'xlm-roberta-ner'...\n",
            "remote: Enumerating objects: 312, done.\u001b[K\n",
            "remote: Counting objects: 100% (312/312), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 312 (delta 165), reused 245 (delta 118), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (312/312), 2.89 MiB | 10.43 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n",
            "Cloning into 'turku-ner-corpus'...\n",
            "remote: Enumerating objects: 1611, done.\u001b[K\n",
            "remote: Counting objects: 100% (1611/1611), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1515/1515), done.\u001b[K\n",
            "remote: Total 1611 (delta 67), reused 1574 (delta 46), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1611/1611), 6.77 MiB | 13.13 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n",
            "Cloning into 'EstNER'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 8 (delta 2), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/mukhal/xlm-roberta-ner.git \n",
        "! git clone https://github.com/TurkuNLP/turku-ner-corpus\n",
        "! git clone https://github.com/ksirts/EstNER"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "YjxbNj70lt-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation to uniform formats across languages. As we will use the XLM-R finetuning code with the English dataset included, we don't have to worry about the format of English data.\n",
        "\n",
        "The expected dataset format is a space-separated file where only the first and last column are looked at (word and its label), other columns are ignored. Sentences are separated by an empty line.\n",
        "\n",
        "Finnish is easy, as it is a .tsv file with those two columns. Estonian is a completely different hierarchical JSON format and needs most preparation.\n",
        "\n",
        "\n",
        "Additionally, we'll limit the named entity labels to just persons (PER), organizations (ORG) and locations (LOC), as those are common in all datasets."
      ],
      "metadata": {
        "id": "rqSYudXnPzlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json"
      ],
      "metadata": {
        "id": "iPQyUII3439f"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir et-data en-data fi-data merged-data"
      ],
      "metadata": {
        "id": "wCv5736ivfM6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allowed_labels = ['O', 'B-LOC', 'I-LOC', 'B-ORG','I-ORG','B-PER','I-PER']"
      ],
      "metadata": {
        "id": "jNbqSQPKEVpK"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estonian:"
      ],
      "metadata": {
        "id": "upqbGsDowBbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for split in [\"dev\",\"test\",\"train\"]:\n",
        "  with open(f\"EstNER/EstNER_v1_{split}.json\", 'r') as f_in:\n",
        "    data = json.loads(f_in.read())\n",
        "  \n",
        "  split = 'valid' if split=='dev' else split\n",
        "\n",
        "  with open(f\"et-data/{split}.txt\", 'w') as f_out:\n",
        "    for document in data:\n",
        "      for sentence in document:\n",
        "        for token in sentence:\n",
        "          # Estonian has multi-part names sometimes marked as one token (e.g. New York). Those must be split to multiple rows to conform with the file format.\n",
        "          words = token['word'].split()\n",
        "          label = token['ner_1']\n",
        "\n",
        "          label = label if label in allowed_labels else 'O'\n",
        "\n",
        "          f_out.write(f\"{words.pop(0)} {label}\\n\")\n",
        "\n",
        "          if words: # name was multipart\n",
        "            # if the first word is a named entity start (label B-*), others must be continuations\n",
        "            if label.split('-')[0]=='B':\n",
        "              label=f\"I-{label.split('-')[1]}\"\n",
        "            \n",
        "            while words:\n",
        "              f_out.write(f\"{words.pop(0)} {label}\\n\")\n",
        "        f_out.write('\\n')"
      ],
      "metadata": {
        "id": "2LDixGH_55dI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finnish:"
      ],
      "metadata": {
        "id": "zt-WTCANF9oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for split in  [\"dev\",\"test\",\"train\"]:\n",
        "  with open(f\"turku-ner-corpus/data/conll/{split}.tsv\", 'r') as f_in:\n",
        "    data = f_in.readlines()\n",
        "  \n",
        "  split = 'valid' if split=='dev' else split\n",
        "\n",
        "  with open(f\"fi-data/{split}.txt\", 'w') as f_out:\n",
        "    for line in data:\n",
        "      columns = line.strip().split('\\t')\n",
        "      if len(columns) >= 2:\n",
        "        if columns[-1] in allowed_labels:\n",
        "          f_out.write(f\"{columns[0]} {columns[-1]}\\n\")\n",
        "        else:\n",
        "          f_out.write(f\"{columns[0]} O\\n\")\n",
        "      else:\n",
        "        f_out.write(f\"\\n\")\n",
        "    f_out.write('\\n')"
      ],
      "metadata": {
        "id": "bR8umK84wKV6"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "English:"
      ],
      "metadata": {
        "id": "KKb2SUTZHyFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for split in  [\"valid\",\"test\",\"train\"]:\n",
        "  with open(f\"xlm-roberta-ner/data/coNLL-2003/{split}.txt\", 'r') as f_in:\n",
        "    data = f_in.readlines()\n",
        "\n",
        "  with open(f\"en-data/{split}.txt\", 'w') as f_out:\n",
        "    for line in data:\n",
        "      columns = line.strip().split()\n",
        "      if len(columns) >= 2:\n",
        "        if columns[-1] in allowed_labels:\n",
        "          f_out.write(f\"{columns[0]} {columns[-1]}\\n\")\n",
        "        else:\n",
        "          f_out.write(f\"{columns[0]} O\\n\")\n",
        "      else:\n",
        "        f_out.write(f\"\\n\")\n",
        "    f_out.write('\\n')"
      ],
      "metadata": {
        "id": "IDaVmHuzHxQF"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create merged datasets:"
      ],
      "metadata": {
        "id": "m4cY1Q9YISQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! for f in {valid,test,train}; do cat {et,en,fi}-data/$f.txt > merged-data/$f.txt; done"
      ],
      "metadata": {
        "id": "X4tzavveWwWl"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of tags across languages. We'll keep only PER, ORG, and LOC - these are supported by all languages."
      ],
      "metadata": {
        "id": "D5NCs2vumNfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning XLM-R"
      ],
      "metadata": {
        "id": "Wr1gdK9ysPzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r xlm-roberta-ner/requirements.txt\n",
        "! pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qChTkDqzTQlr",
        "outputId": "b8f2820e-d0f5-4e2f-fceb-4e061f9821cf"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq==0.9.0\n",
            "  Downloading fairseq-0.9.0.tar.gz (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers==1.2.0\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 52.1 MB/s \n",
            "\u001b[?25hCollecting seqeval==0.0.12\n",
            "  Downloading seqeval-0.0.12.tar.gz (21 kB)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r xlm-roberta-ner/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r xlm-roberta-ner/requirements.txt (line 1)) (0.29.24)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r xlm-roberta-ner/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r xlm-roberta-ner/requirements.txt (line 1)) (2019.12.20)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r xlm-roberta-ner/requirements.txt (line 1)) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r xlm-roberta-ner/requirements.txt (line 1)) (4.62.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0->-r xlm-roberta-ner/requirements.txt (line 2)) (2.23.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.23-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 72.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12->-r xlm-roberta-ner/requirements.txt (line 3)) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq==0.9.0->-r xlm-roberta-ner/requirements.txt (line 1)) (3.10.0.2)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.24.0,>=1.23.23\n",
            "  Downloading botocore-1.23.23-py3-none-any.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 28.9 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.23->boto3->pytorch-transformers==1.2.0->-r xlm-roberta-ner/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.23->boto3->pytorch-transformers==1.2.0->-r xlm-roberta-ner/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.9.0->-r xlm-roberta-ner/requirements.txt (line 1)) (2.21)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0->-r xlm-roberta-ner/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0->-r xlm-roberta-ner/requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0->-r xlm-roberta-ner/requirements.txt (line 2)) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 51.1 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->fairseq==0.9.0->-r xlm-roberta-ner/requirements.txt (line 1)) (0.8.9)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers==1.2.0->-r xlm-roberta-ner/requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers==1.2.0->-r xlm-roberta-ner/requirements.txt (line 2)) (7.1.2)\n",
            "Building wheels for collected packages: fairseq, seqeval\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp37-cp37m-linux_x86_64.whl size=2168263 sha256=b11f82f8769e2f488162f7fe252b31434db1cf5726bfc4d29602d7ae48ab00ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/27/e2/c55614da7eb71041bb08f02e8a302b869e51185eb7c575a604\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7434 sha256=27a609c4aed5737504d1acc7457465165b9fc9633d151757b0088b3ce48657c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/cc/62/a3b81f92d35a80e39eb9b2a9d8b31abac54c02b21b2d466edc\n",
            "Successfully built fairseq seqeval\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, portalocker, colorama, sentencepiece, sacremoses, sacrebleu, boto3, seqeval, pytorch-transformers, fairseq\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.23 botocore-1.23.23 colorama-0.4.4 fairseq-0.9.0 jmespath-0.10.0 portalocker-2.3.2 pytorch-transformers-1.2.0 s3transfer-0.5.0 sacrebleu-2.0.0 sacremoses-0.0.46 sentencepiece-0.1.96 seqeval-0.0.12 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.0-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=59e550096cf29e0381001c1cbe804e66b058d918bbf2d052c3de5097aa754801\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=f3839d1c1d98183b3a997913bab201d6181db4a0bfea5e2eb107305efd2162c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.24 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.0 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.7 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking which GPU resources we have:"
      ],
      "metadata": {
        "id": "rKtF-OAHqigx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF7-G1euttAx",
        "outputId": "6d797e99-9b59-47c0-accb-93857fd7ab70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 11 13:37:56 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the pretrained XLM-R model"
      ],
      "metadata": {
        "id": "uMqBQYllqzNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir model_dir\n",
        "! mkdir pretrained_models \n",
        "! wget -P pretrained_models https://dl.fbaipublicfiles.com/fairseq/models/xlmr.base.tar.gz\n",
        "! tar xzvf pretrained_models/xlmr.base.tar.gz  --directory pretrained_models/\n",
        "! rm -r pretrained_models/xlmr.base.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fEsY8ortD0D",
        "outputId": "4161f9f6-22f3-4ce8-eea0-4bfde62860f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘pretrained_models’: File exists\n",
            "--2021-12-11 13:36:10--  https://dl.fbaipublicfiles.com/fairseq/models/xlmr.base.tar.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 512274718 (489M) [application/gzip]\n",
            "Saving to: ‘pretrained_models/xlmr.base.tar.gz’\n",
            "\n",
            "xlmr.base.tar.gz    100%[===================>] 488.54M  32.0MB/s    in 16s     \n",
            "\n",
            "2021-12-11 13:36:26 (31.2 MB/s) - ‘pretrained_models/xlmr.base.tar.gz’ saved [512274718/512274718]\n",
            "\n",
            "xlmr.base/\n",
            "xlmr.base/dict.txt\n",
            "xlmr.base/sentencepiece.bpe.model\n",
            "xlmr.base/model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up Weights & Biases monitoring to keep an eye on GPU performance metrics (utilization, memory consumption, etc.)"
      ],
      "metadata": {
        "id": "OwAT3934q6xU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ySi_nW9RvPwg",
        "outputId": "e178f052-f452-434d-8351-2dba4076fbc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliisaratsep\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/liisaratsep/uncategorized/runs/22fwr3bu\" target=\"_blank\">vivid-hill-2</a></strong> to <a href=\"https://wandb.ai/liisaratsep/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7faffa819190>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/liisaratsep/uncategorized/runs/22fwr3bu?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning with the same parameters as close to the original [XLM-R paper](https://arxiv.org/pdf/1911.02116.pdf) as possible. This finetunes the multilingual model, by changin the `data_dir` path, finetuning on a single language is possible.\n",
        "\n",
        "PS: actual finetuning was done on UT HPC Rocket cluster, as it was faster, but the example below is completely functional."
      ],
      "metadata": {
        "id": "2tN8UA2uraCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python xlm-roberta-ner/main.py \\\n",
        "    --data_dir=./merged-data/  \\\n",
        "    --task_name=ner   \\\n",
        "    --output_dir=merged-model/   \\\n",
        "    --max_seq_length=128   \\\n",
        "    --num_train_epochs 10  \\\n",
        "    --do_eval \\\n",
        "    --warmup_proportion=0.0 \\\n",
        "    --pretrained_path pretrained_models/xlmr.base/ \\\n",
        "    --learning_rate 6e-5 \\\n",
        "    --do_train \\\n",
        "    --eval_on dev \\\n",
        "    --train_batch_size 32 \\\n",
        "    --dropout 0.2 \\\n",
        "    --train_batch_size 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnAa3kjEtZ1V",
        "outputId": "9ab4c392-ceb8-486a-d1f3-6bab37b05737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading archive file pretrained_models/xlmr.base/\n",
            "| dictionary: 250001 types\n",
            "12/11/2021 14:10:54 - INFO - root -   *** Example ***\n",
            "12/11/2021 14:10:54 - INFO - root -   guid: train-0\n",
            "12/11/2021 14:10:54 - INFO - root -   tokens: 0 32232 107272 433 11532 22869 24317 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/11/2021 14:10:54 - INFO - root -   input_ids: 0 32232 107272 433 11532 22869 24317 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/11/2021 14:10:54 - INFO - root -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2021 14:10:54 - INFO - root -   label: ['O', 'O', 'O'] (id = 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)\n",
            "12/11/2021 14:10:54 - INFO - root -   label_mask: 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2021 14:10:54 - INFO - root -   valid mask: 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2021 14:10:54 - INFO - root -   *** Example ***\n",
            "12/11/2021 14:10:54 - INFO - root -   guid: train-1\n",
            "12/11/2021 14:10:54 - INFO - root -   tokens: 0 150697 227223 50577 130895 70862 141 228519 1277 67567 1224 84748 3187 107272 433 11532 1194 16320 46218 31955 182 202 95860 20316 6 4 130678 275 28215 153406 57 11532 132448 814 6 60272 68347 143 1277 47818 20740 13 6 4 33459 11226 17996 44235 19892 16320 46218 18764 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/11/2021 14:10:54 - INFO - root -   input_ids: 0 150697 227223 50577 130895 70862 141 228519 1277 67567 1224 84748 3187 107272 433 11532 1194 16320 46218 31955 182 202 95860 20316 6 4 130678 275 28215 153406 57 11532 132448 814 6 60272 68347 143 1277 47818 20740 13 6 4 33459 11226 17996 44235 19892 16320 46218 18764 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/11/2021 14:10:54 - INFO - root -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2021 14:10:54 - INFO - root -   label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'] (id = 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)\n",
            "12/11/2021 14:10:54 - INFO - root -   label_mask: 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2021 14:10:54 - INFO - root -   valid mask: 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2021 14:11:29 - INFO - __main__ -   ***** Running training *****\n",
            "12/11/2021 14:11:29 - INFO - __main__ -     Num examples = 37167\n",
            "12/11/2021 14:11:29 - INFO - __main__ -     Batch size = 32\n",
            "12/11/2021 14:11:29 - INFO - __main__ -     Num steps = 11610\n",
            "12/11/2021 14:11:30 - INFO - root -   *** Example ***\n",
            "12/11/2021 14:11:30 - INFO - root -   guid: valid-0\n",
            "12/11/2021 14:11:30 - INFO - root -   tokens: 0 164473 184 763 86690 7 23420 6153 133 5151 32781 43106 2663 6704 7 51587 2179 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/11/2021 14:11:30 - INFO - root -   input_ids: 0 164473 184 763 86690 7 23420 6153 133 5151 32781 43106 2663 6704 7 51587 2179 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/11/2021 14:11:30 - INFO - root -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2021 14:11:30 - INFO - root -   label: ['B-LOC', 'I-LOC', 'O', 'O', 'O'] (id = 0 6 0 0 7 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)\n",
            "12/11/2021 14:11:30 - INFO - root -   label_mask: 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2021 14:11:30 - INFO - root -   valid mask: 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2021 14:11:30 - INFO - root -   *** Example ***\n",
            "12/11/2021 14:11:30 - INFO - root -   guid: valid-1\n",
            "12/11/2021 14:11:30 - INFO - root -   tokens: 0 25714 670 62 6371 92614 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/11/2021 14:11:30 - INFO - root -   input_ids: 0 25714 670 62 6371 92614 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/11/2021 14:11:30 - INFO - root -   input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2021 14:11:30 - INFO - root -   label: ['B-PER', 'I-PER'] (id = 0 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)\n",
            "12/11/2021 14:11:30 - INFO - root -   label_mask: 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/11/2021 14:11:30 - INFO - root -   valid mask: 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "xlm-roberta-ner/main.py:147: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for _ in tqdm(range(args.num_train_epochs), desc=\"Epoch\"):\n",
            "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
            "xlm-roberta-ner/main.py:151: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  tbar = tqdm(train_dataloader, desc=\"Iteration\")\n",
            "Iteration:   0%|          | 0/1162 [00:00<?, ?it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "12/11/2021 14:37:11 - INFO - __main__ -   \n",
            "Testing on validation set...\n",
            "12/11/2021 14:39:07 - INFO - __main__ -   \n",
            "Found better f1=0.8820 on validation set. Saving model\n",
            "\n",
            "12/11/2021 14:39:07 - INFO - __main__ -              precision    recall  f1-score   support\n",
            "\n",
            "      PER     0.9236    0.9283    0.9260      2775\n",
            "      LOC     0.8833    0.9165    0.8996      2239\n",
            "      ORG     0.7771    0.8397    0.8072      2084\n",
            "\n",
            "micro avg     0.8661    0.8986    0.8820      7098\n",
            "macro avg     0.8679    0.8986    0.8828      7098\n",
            "\n",
            "\n",
            "Iteration:   0%|          | 0/1162 [00:00<?, ?it/s]\n",
            "12/11/2021 15:04:52 - INFO - __main__ -   \n",
            "Testing on validation set...\n",
            "12/11/2021 15:06:46 - INFO - __main__ -   \n",
            "Found better f1=0.8984 on validation set. Saving model\n",
            "\n",
            "12/11/2021 15:06:46 - INFO - __main__ -              precision    recall  f1-score   support\n",
            "\n",
            "      PER     0.9071    0.9643    0.9348      2775\n",
            "      LOC     0.9569    0.8821    0.9180      2239\n",
            "      ORG     0.8093    0.8512    0.8297      2084\n",
            "\n",
            "micro avg     0.8916    0.9052    0.8984      7098\n",
            "macro avg     0.8941    0.9052    0.8987      7098\n",
            "\n",
            "\n",
            "Iteration:   0%|          | 0/1162 [00:00<?, ?it/s]\n",
            "12/11/2021 15:32:28 - INFO - __main__ -   \n",
            "Testing on validation set...\n",
            "12/11/2021 15:34:23 - INFO - __main__ -   \n",
            "Found better f1=0.9058 on validation set. Saving model\n",
            "\n",
            "12/11/2021 15:34:23 - INFO - __main__ -              precision    recall  f1-score   support\n",
            "\n",
            "      PER     0.9398    0.9452    0.9425      2775\n",
            "      LOC     0.8868    0.9477    0.9162      2239\n",
            "      ORG     0.8542    0.8349    0.8445      2084\n",
            "\n",
            "micro avg     0.8981    0.9136    0.9058      7098\n",
            "macro avg     0.8979    0.9136    0.9054      7098\n",
            "\n",
            "\n",
            "Iteration:   0%|          | 0/1162 [00:00<?, ?it/s]\n",
            "12/11/2021 16:00:08 - INFO - __main__ -   \n",
            "Testing on validation set...\n",
            "12/11/2021 16:02:05 - INFO - __main__ -   \n",
            "Found better f1=0.9093 on validation set. Saving model\n",
            "\n",
            "12/11/2021 16:02:05 - INFO - __main__ -              precision    recall  f1-score   support\n",
            "\n",
            "      PER     0.9456    0.9402    0.9429      2775\n",
            "      LOC     0.9309    0.9272    0.9291      2239\n",
            "      ORG     0.8474    0.8393    0.8433      2084\n",
            "\n",
            "micro avg     0.9122    0.9065    0.9093      7098\n",
            "macro avg     0.9122    0.9065    0.9093      7098\n",
            "\n",
            "\n",
            "Iteration:   0%|          | 0/1162 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"xlm-roberta-ner/main.py\", line 228, in <module>\n",
            "    main()\n",
            "  File \"xlm-roberta-ner/main.py\", line 157, in main\n",
            "    loss = model(input_ids, label_ids, l_mask, valid_ids)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/xlm-roberta-ner/model/xlmr_for_token_classification.py\", line 53, in forward\n",
            "    active_logits = logits.view(-1, self.n_labels)[active_loss]\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ]
}