{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "train-ner",
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyNkoW+gODmMvB8mdz4sijvf",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mirjampaales/cool-ml-project/blob/main/named_entity_recognition/train_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Downloading data \n",
    "This can be skipped if using the notebook withing the .git repository as the repositories below are included there as submodules:"
   ],
   "metadata": {
    "id": "3pkeTfcN3xWq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "it8NGdCn2_l4",
    "outputId": "f54c33ed-8f59-4e4c-e8d7-ca5e751aa2ed"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'xlm-roberta-ner'...\n",
      "remote: Enumerating objects: 312, done.\u001B[K\n",
      "remote: Counting objects: 100% (312/312), done.\u001B[K\n",
      "remote: Compressing objects: 100% (187/187), done.\u001B[K\n",
      "remote: Total 312 (delta 165), reused 245 (delta 118), pack-reused 0\u001B[K\n",
      "Receiving objects: 100% (312/312), 2.89 MiB | 10.43 MiB/s, done.\n",
      "Resolving deltas: 100% (165/165), done.\n",
      "Cloning into 'turku-ner-corpus'...\n",
      "remote: Enumerating objects: 1611, done.\u001B[K\n",
      "remote: Counting objects: 100% (1611/1611), done.\u001B[K\n",
      "remote: Compressing objects: 100% (1515/1515), done.\u001B[K\n",
      "remote: Total 1611 (delta 67), reused 1574 (delta 46), pack-reused 0\u001B[K\n",
      "Receiving objects: 100% (1611/1611), 6.77 MiB | 13.13 MiB/s, done.\n",
      "Resolving deltas: 100% (67/67), done.\n",
      "Cloning into 'EstNER'...\n",
      "remote: Enumerating objects: 8, done.\u001B[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001B[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001B[K\n",
      "remote: Total 8 (delta 2), reused 4 (delta 1), pack-reused 0\u001B[K\n",
      "Unpacking objects: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/mukhal/xlm-roberta-ner.git \n",
    "! git clone https://github.com/TurkuNLP/turku-ner-corpus\n",
    "! git clone https://github.com/ksirts/EstNER"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation"
   ],
   "metadata": {
    "id": "YjxbNj70lt-r"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data preparation to uniform formats across languages. As we will use the XLM-R finetuning code with the English dataset included, we don't have to worry about the format of English data.\n",
    "\n",
    "The expected dataset format is a space-separated file where only the first and last column are looked at (word and its label), other columns are ignored. Sentences are separated by an empty line.\n",
    "\n",
    "Finnish is easy, as it is a .tsv file with those two columns. Estonian is a completely different hierarchical JSON format and needs most preparation.\n",
    "\n",
    "\n",
    "Additionally, we'll limit the named entity labels to just persons (PER), organizations (ORG) and locations (LOC), as those are common in all datasets."
   ],
   "metadata": {
    "id": "rqSYudXnPzlX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json"
   ],
   "metadata": {
    "id": "iPQyUII3439f"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "! cd data; mkdir et en fi merged"
   ],
   "metadata": {
    "id": "wCv5736ivfM6"
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "allowed_labels = ['O', 'B-LOC', 'I-LOC', 'B-ORG','I-ORG','B-PER','I-PER']"
   ],
   "metadata": {
    "id": "jNbqSQPKEVpK"
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Estonian:"
   ],
   "metadata": {
    "id": "upqbGsDowBbG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for split in [\"dev\",\"test\",\"train\"]:\n",
    "  with open(f\"EstNER/EstNER_v1_{split}.json\", 'r') as f_in:\n",
    "    data = json.loads(f_in.read())\n",
    "  \n",
    "  split = 'valid' if split=='dev' else split\n",
    "\n",
    "  with open(f\"data/et/{split}.txt\", 'w') as f_out:\n",
    "    for document in data:\n",
    "      for sentence in document:\n",
    "        for token in sentence:\n",
    "          # Estonian has multi-part names sometimes marked as one token (e.g. New York). Those must be split to multiple rows to conform with the file format.\n",
    "          words = token['word'].split()\n",
    "          label = token['ner_1']\n",
    "\n",
    "          label = label if label in allowed_labels else 'O'\n",
    "\n",
    "          f_out.write(f\"{words.pop(0)} {label}\\n\")\n",
    "\n",
    "          if words: # name was multipart\n",
    "            # if the first word is a named entity start (label B-*), others must be continuations\n",
    "            if label.split('-')[0]=='B':\n",
    "              label=f\"I-{label.split('-')[1]}\"\n",
    "            \n",
    "            while words:\n",
    "              f_out.write(f\"{words.pop(0)} {label}\\n\")\n",
    "        f_out.write('\\n')"
   ],
   "metadata": {
    "id": "2LDixGH_55dI"
   },
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finnish:"
   ],
   "metadata": {
    "id": "zt-WTCANF9oG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for split in  [\"dev\",\"test\",\"train\"]:\n",
    "  with open(f\"turku-ner-corpus/data/conll/{split}.tsv\", 'r') as f_in:\n",
    "    data = f_in.readlines()\n",
    "  \n",
    "  split = 'valid' if split=='dev' else split\n",
    "\n",
    "  with open(f\"data/fi/{split}.txt\", 'w') as f_out:\n",
    "    for line in data:\n",
    "      columns = line.strip().split('\\t')\n",
    "      if len(columns) >= 2:\n",
    "        if columns[-1] in allowed_labels:\n",
    "          f_out.write(f\"{columns[0]} {columns[-1]}\\n\")\n",
    "        else:\n",
    "          f_out.write(f\"{columns[0]} O\\n\")\n",
    "      else:\n",
    "        f_out.write(f\"\\n\")\n",
    "    f_out.write('\\n')"
   ],
   "metadata": {
    "id": "bR8umK84wKV6"
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "English:"
   ],
   "metadata": {
    "id": "KKb2SUTZHyFH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for split in  [\"valid\",\"test\",\"train\"]:\n",
    "  with open(f\"xlm-roberta-ner/data/coNLL-2003/{split}.txt\", 'r') as f_in:\n",
    "    data = f_in.readlines()\n",
    "\n",
    "  with open(f\"data/en/{split}.txt\", 'w') as f_out:\n",
    "    for line in data:\n",
    "      columns = line.strip().split()\n",
    "      if len(columns) >= 2:\n",
    "        if columns[-1] in allowed_labels:\n",
    "          f_out.write(f\"{columns[0]} {columns[-1]}\\n\")\n",
    "        else:\n",
    "          f_out.write(f\"{columns[0]} O\\n\")\n",
    "      else:\n",
    "        f_out.write(f\"\\n\")\n",
    "    f_out.write('\\n')"
   ],
   "metadata": {
    "id": "IDaVmHuzHxQF"
   },
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create merged datasets:"
   ],
   "metadata": {
    "id": "m4cY1Q9YISQU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! for f in {valid,test,train}; do cat data/{et,en,fi}/$f.txt > data/merged/$f.txt; done"
   ],
   "metadata": {
    "id": "X4tzavveWwWl"
   },
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finetuning XLM-R"
   ],
   "metadata": {
    "id": "Wr1gdK9ysPzj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install -r xlm-roberta-ner/requirements.txt\n",
    "! pip install wandb"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qChTkDqzTQlr",
    "outputId": "b8f2820e-d0f5-4e2f-fceb-4e061f9821cf"
   },
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking which GPU resources we have:"
   ],
   "metadata": {
    "id": "rKtF-OAHqigx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MF7-G1euttAx",
    "outputId": "6d797e99-9b59-47c0-accb-93857fd7ab70"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downloading the pretrained XLM-R model"
   ],
   "metadata": {
    "id": "uMqBQYllqzNb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! mkdir model_dir\n",
    "! mkdir pretrained_models finetuned_models\n",
    "! wget -P pretrained_models https://dl.fbaipublicfiles.com/fairseq/models/xlmr.base.tar.gz\n",
    "! tar xzvf pretrained_models/xlmr.base.tar.gz  --directory pretrained_models/\n",
    "! rm -r pretrained_models/xlmr.base.tar.gz"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fEsY8ortD0D",
    "outputId": "4161f9f6-22f3-4ce8-eea0-4bfde62860f1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up Weights & Biases monitoring to keep an eye on GPU performance metrics (utilization, memory consumption, etc.)"
   ],
   "metadata": {
    "id": "OwAT3934q6xU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.init()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "ySi_nW9RvPwg",
    "outputId": "e178f052-f452-434d-8351-2dba4076fbc9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mliisaratsep\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/liisaratsep/uncategorized/runs/22fwr3bu\" target=\"_blank\">vivid-hill-2</a></strong> to <a href=\"https://wandb.ai/liisaratsep/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7faffa819190>"
      ],
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/liisaratsep/uncategorized/runs/22fwr3bu?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finetuning with the same parameters as close to the original [XLM-R paper](https://arxiv.org/pdf/1911.02116.pdf) as \n",
    "possible.\n",
    "\n",
    "PS: actual finetuning was done on UT HPC Rocket cluster, as it was faster, but the examples below are completely \n",
    "functional."
   ],
   "metadata": {
    "id": "2tN8UA2uraCm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# multilingual finetuning\n",
    "\n",
    "! python xlm-roberta-ner/main.py \\\n",
    "    --data_dir=./data/merged/  \\\n",
    "    --task_name=ner   \\\n",
    "    --output_dir=finetuned_models/merged-base/   \\\n",
    "    --max_seq_length=128   \\\n",
    "    --num_train_epochs 10  \\\n",
    "    --do_eval \\\n",
    "    --warmup_proportion=0.0 \\\n",
    "    --pretrained_path pretrained_models/xlmr.base/ \\\n",
    "    --learning_rate 6e-5 \\\n",
    "    --do_train \\\n",
    "    --eval_on dev \\\n",
    "    --dropout 0.2 \\\n",
    "    --train_batch_size 32"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnAa3kjEtZ1V",
    "outputId": "9ab4c392-ceb8-486a-d1f3-6bab37b05737"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ET finetuning\n",
    "\n",
    "! python xlm-roberta-ner/main.py \\\n",
    "    --data_dir=./data/et/  \\\n",
    "    --task_name=ner   \\\n",
    "    --output_dir=finetuned_models/et-base/   \\\n",
    "    --max_seq_length=128   \\\n",
    "    --num_train_epochs 10  \\\n",
    "    --do_eval \\\n",
    "    --warmup_proportion=0.0 \\\n",
    "    --pretrained_path pretrained_models/xlmr.base/ \\\n",
    "    --learning_rate 6e-5 \\\n",
    "    --do_train \\\n",
    "    --eval_on dev \\\n",
    "    --dropout 0.2 \\\n",
    "    --train_batch_size 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# EN finetuning\n",
    "\n",
    "! python xlm-roberta-ner/main.py \\\n",
    "    --data_dir=./data/en/  \\\n",
    "    --task_name=ner   \\\n",
    "    --output_dir=finetuned_models/et-base/   \\\n",
    "    --max_seq_length=128   \\\n",
    "    --num_train_epochs 10  \\\n",
    "    --do_eval \\\n",
    "    --warmup_proportion=0.0 \\\n",
    "    --pretrained_path pretrained_models/xlmr.base/ \\\n",
    "    --learning_rate 6e-5 \\\n",
    "    --do_train \\\n",
    "    --eval_on dev \\\n",
    "    --dropout 0.2 \\\n",
    "    --train_batch_size 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FI finetuning\n",
    "\n",
    "! python xlm-roberta-ner/main.py \\\n",
    "    --data_dir=./data/fi/  \\\n",
    "    --task_name=ner   \\\n",
    "    --output_dir=finetuned_models/fi-base/   \\\n",
    "    --max_seq_length=128   \\\n",
    "    --num_train_epochs 10  \\\n",
    "    --do_eval \\\n",
    "    --warmup_proportion=0.0 \\\n",
    "    --pretrained_path pretrained_models/xlmr.base/ \\\n",
    "    --learning_rate 6e-5 \\\n",
    "    --do_train \\\n",
    "    --eval_on dev \\\n",
    "    --dropout 0.2 \\\n",
    "    --train_batch_size 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}