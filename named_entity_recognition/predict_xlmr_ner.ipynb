{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict-xlmr-ner",
      "provenance": [],
      "mount_file_id": "14dDlW-CP1H6_dpDfgiUKdjD-Xu6khRfN",
      "authorship_tag": "ABX9TyMC/GqEPMj1/1RULqouJzIw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirjampaales/cool-ml-project/blob/main/named_entity_recognition/predict_xlmr_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Environment setup"
      ],
      "metadata": {
        "id": "3pkeTfcN3xWq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it8NGdCn2_l4",
        "outputId": "e0db811b-a4a6-4dcc-e4fe-75e795dfe046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'xlm-roberta-ner'...\n",
            "remote: Enumerating objects: 312, done.\u001b[K\n",
            "remote: Counting objects: 100% (312/312), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 312 (delta 165), reused 245 (delta 118), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (312/312), 2.89 MiB | 9.71 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/mukhal/xlm-roberta-ner.git "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r xlm-roberta-ner/requirements.txt"
      ],
      "metadata": {
        "id": "qChTkDqzTQlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the pretrained XLM-R model"
      ],
      "metadata": {
        "id": "uMqBQYllqzNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd xlm-roberta-ner\n",
        "\n",
        "! mkdir pretrained_models \n",
        "! wget -P pretrained_models https://dl.fbaipublicfiles.com/fairseq/models/xlmr.base.tar.gz\n",
        "! tar xzvf pretrained_models/xlmr.base.tar.gz  --directory pretrained_models/\n",
        "! rm -r pretrained_models/xlmr.base.tar.gz"
      ],
      "metadata": {
        "id": "6fEsY8ortD0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229f1ecf-cd98-44d7-e35c-5b9aa3e677ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/xlm-roberta-ner\n",
            "--2021-12-13 00:29:55--  https://dl.fbaipublicfiles.com/fairseq/models/xlmr.base.tar.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 512274718 (489M) [application/gzip]\n",
            "Saving to: ‘pretrained_models/xlmr.base.tar.gz’\n",
            "\n",
            "xlmr.base.tar.gz    100%[===================>] 488.54M  34.6MB/s    in 15s     \n",
            "\n",
            "2021-12-13 00:30:10 (33.6 MB/s) - ‘pretrained_models/xlmr.base.tar.gz’ saved [512274718/512274718]\n",
            "\n",
            "xlmr.base/\n",
            "xlmr.base/dict.txt\n",
            "xlmr.base/sentencepiece.bpe.model\n",
            "xlmr.base/model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import SequentialSampler, DataLoader\n",
        "\n",
        "from model.xlmr_for_token_classification import XLMRForTokenClassification\n",
        "from utils.data_utils import InputExample, convert_examples_to_features, InputFeatures, NerProcessor, create_dataset"
      ],
      "metadata": {
        "id": "cR6tKW_9MyHD"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model loading\n",
        "\n",
        "This sample assumes a model which finetuned XLM-R base is used. If not, then a different pretrained model should be downloaded and the hidden_size parameter may need to be changed. The path of the finetuned model should be configured.\n",
        "\n",
        "Multilingually finetuned base model can be found [here](https://drive.google.com/file/d/1vVRnEup8AEoUEp1XehV52zkAr0BEwRry/view?usp=sharing)"
      ],
      "metadata": {
        "id": "aGbJxC70SZkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
        "\n",
        "processor = NerProcessor()\n",
        "label_list = processor.get_labels()\n",
        "num_labels = len(label_list) + 1"
      ],
      "metadata": {
        "id": "OAvRXi2gNAcP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XLMRForTokenClassification(pretrained_path='pretrained_models/xlmr.base/',\n",
        "                                    n_labels=num_labels, hidden_size=768,\n",
        "                                    dropout_p=0, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VCwtTFaNAH0",
        "outputId": "0e098a0c-6c93-4304-e598-f974b56a8ca5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading archive file pretrained_models/xlmr.base/\n",
            "| dictionary: 250001 types\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('/content/drive/MyDrive/Colab Notebooks/Machine Learning (Fall 2021)/model.pt', map_location=torch.device(device))\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "mO4kJ2hiTNTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference methods"
      ],
      "metadata": {
        "id": "zv7dq6_DS8H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentences, max_seq_length = 128):\n",
        "  examples = [InputExample(text_a=sentence, guid=str(i)) for i, sentence in enumerate(sentences)]\n",
        "\n",
        "  # Generating features\n",
        "  label_map = {label: i for i, label in enumerate(label_list, 1)}\n",
        "  ignored_label = \"IGNORE\"\n",
        "  label_map[ignored_label] = 0  # 0 label is to be ignored\n",
        "  features = []\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "      textlist = example.text_a.split(' ')\n",
        "      labels = []\n",
        "      valid = []\n",
        "      label_mask = []\n",
        "      token_ids = []\n",
        "\n",
        "      for i, word in enumerate(textlist):\n",
        "          tokens = model.encode_word(word.strip())  # word token ids\n",
        "          token_ids.extend(tokens)  # all sentence token ids\n",
        "          label_1 = 'O'\n",
        "          for m in range(len(tokens)):\n",
        "              if m == 0:  # only label the first BPE token of each work\n",
        "                  labels.append(label_1)\n",
        "                  valid.append(1)\n",
        "                  label_mask.append(1)\n",
        "              else:\n",
        "                  labels.append(ignored_label)  # unlabeled BPE token\n",
        "                  label_mask.append(0)\n",
        "                  valid.append(0)\n",
        "\n",
        "      if len(token_ids) >= max_seq_length - 1:  # trim extra tokens\n",
        "          token_ids = token_ids[0:(max_seq_length - 2)]\n",
        "          labels = labels[0:(max_seq_length - 2)]\n",
        "          valid = valid[0:(max_seq_length - 2)]\n",
        "          label_mask = label_mask[0:(max_seq_length - 2)]\n",
        "\n",
        "      # adding <s>\n",
        "      token_ids.insert(0, 0)\n",
        "      labels.insert(0, ignored_label)\n",
        "      label_mask.insert(0, 0)\n",
        "      valid.insert(0, 0)\n",
        "\n",
        "      # adding </s>\n",
        "      token_ids.append(2)\n",
        "      labels.append(ignored_label)\n",
        "      label_mask.append(0)\n",
        "      valid.append(0)\n",
        "\n",
        "      label_ids = []\n",
        "      for i, _ in enumerate(token_ids):\n",
        "          label_ids.append(label_map[labels[i]])\n",
        "\n",
        "      input_mask = [1] * len(token_ids)\n",
        "\n",
        "      while len(token_ids) < max_seq_length:\n",
        "          token_ids.append(1)  # token padding idx\n",
        "          input_mask.append(0)\n",
        "          label_ids.append(label_map[ignored_label])  # label ignore idx\n",
        "          valid.append(0)\n",
        "          label_mask.append(0)\n",
        "\n",
        "      while len(label_ids) < max_seq_length:\n",
        "          label_ids.append(label_map[ignored_label])\n",
        "          label_mask.append(0)\n",
        "\n",
        "      features.append(\n",
        "          InputFeatures(input_ids=token_ids,\n",
        "                        input_mask=input_mask,\n",
        "                        label_id=label_ids,\n",
        "                        valid_ids=valid,\n",
        "                        label_mask=label_mask))\n",
        "      \n",
        "  data = create_dataset(features)\n",
        "\n",
        "  # Predict\n",
        "  eval_sampler = SequentialSampler(data)\n",
        "  eval_dataloader = DataLoader(data, sampler=eval_sampler, batch_size=16)\n",
        "\n",
        "  model.eval()  # turn of dropout\n",
        "\n",
        "  y_pred = []\n",
        "\n",
        "  label_map = {i: label for i, label in enumerate(label_list, 1)}\n",
        "\n",
        "  for input_ids, label_ids, l_mask, valid_ids in eval_dataloader:\n",
        "\n",
        "      input_ids = input_ids.to(device)\n",
        "      label_ids = label_ids.to(device)\n",
        "\n",
        "      valid_ids = valid_ids.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          logits = model(input_ids, labels=None, labels_mask=None, valid_mask=valid_ids)\n",
        "\n",
        "      logits = torch.argmax(logits, dim=2)\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = label_ids.cpu().numpy()\n",
        "\n",
        "      for i, cur_label in enumerate(label_ids):\n",
        "          temp = []\n",
        "\n",
        "          for j, m in enumerate(cur_label):\n",
        "              if valid_ids[i][j]:  # if it's a valid label\n",
        "                  temp.append(label_map[logits[i][j]])\n",
        "\n",
        "          assert len(temp) == len(temp)\n",
        "          y_pred.append(temp)\n",
        "  \n",
        "  return [example.text_a.split(' ') for example in examples], y_pred"
      ],
      "metadata": {
        "id": "hf2XyrfgS_DZ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_names(sentences):\n",
        "  tokens, labels = predict(sentences)\n",
        "  per = []\n",
        "  org = []\n",
        "  loc = []\n",
        "\n",
        "  for sent_tokens, sent_labels in zip(tokens, labels):\n",
        "    sentence = ' '.join(sent_tokens)\n",
        "    sent_per = []\n",
        "    sent_org = []\n",
        "    sent_loc = []\n",
        "\n",
        "    entities = {\n",
        "        'PER': sent_per,\n",
        "        'ORG': sent_org,\n",
        "        'LOC': sent_loc\n",
        "    }\n",
        "\n",
        "    temp_entity = []\n",
        "    temp_label = None\n",
        "    for token, label in zip(sent_tokens, sent_labels):\n",
        "      if label.split('-')[0] == 'B':\n",
        "        temp_entity.append(token)\n",
        "        temp_label = label.split('-')[1]\n",
        "      elif label.split('-')[0] == 'I' and label.split('-')[1] == temp_label:\n",
        "        temp_entity.append(token)\n",
        "      else:\n",
        "        if temp_label is not None:\n",
        "          entities[temp_label].append(' '.join(temp_entity))\n",
        "          temp_entity = []\n",
        "          temp_label = None\n",
        "\n",
        "    if temp_label is not None:\n",
        "          entities[temp_label].append(' '.join(temp_entity))\n",
        "    \n",
        "    per.append(sent_per)\n",
        "    org.append(sent_org)\n",
        "    loc.append(sent_loc)\n",
        "  \n",
        "  return per, org, loc"
      ],
      "metadata": {
        "id": "XVWnwLVUU9Jb"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction\n",
        "\n",
        "Can be applied on any file with one sentence per line."
      ],
      "metadata": {
        "id": "nPZOEwkqahvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('../test.txt', sep=\"\\n\", header=None, names=['sentence'])"
      ],
      "metadata": {
        "id": "MCelAfBaMXMD"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per, org, loc = extract_names(df['sentence'].tolist())"
      ],
      "metadata": {
        "id": "QiKsV1Q4boVb"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['per'] = per\n",
        "df['org'] = org\n",
        "df['loc'] = loc"
      ],
      "metadata": {
        "id": "SH4aZaP2dc5_"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('../ner.csv', index=False)"
      ],
      "metadata": {
        "id": "YbPqPreLbBVP"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "R0oxTHKffWL3",
        "outputId": "051c6a8a-2b37-4ca2-b863-df4853821693"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>per</th>\n",
              "      <th>org</th>\n",
              "      <th>loc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Main entrance from the Petser monastery</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Petser]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>View Tallinn from the Old Kopli Road</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Old Kopli Road]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Photo postcard</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tallinn : Aleksander Nevski Cathedral</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Aleksander Nevski Cathedral]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reval : Strandpforten installation</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>Talvinen näkymä Antinkadulta (=Lönnrotinkatu),...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Helsingin Suomalaisen Reaalilyseon, Ressun lu...</td>\n",
              "      <td>[Antinkadulta, Vanhan kirkon, Kirkkotoria]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>Eerikinkatu 6, 4, 2. Taustalla Yrjönkatu 25.</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Eerikinkatu, Yrjönkatu]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>Vappukulkue ylittämässä Pitkääsiltaa matkalla ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Pitkääsiltaa, Stadionille.]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>Pallopeliä pelataan Kalliolinnassa. Kalliolinn...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Kalliolinnassa. Kalliolinnantie]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>Autojen pysäköintoalue Kampin kolmiossa, Runeb...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Kampin, Runeberginkadun, Malminkadun, Malmink...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>116 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sentence  ...                                                loc\n",
              "0              Main entrance from the Petser monastery  ...                                           [Petser]\n",
              "1                 View Tallinn from the Old Kopli Road  ...                                   [Old Kopli Road]\n",
              "2                                       Photo postcard  ...                                                 []\n",
              "3                Tallinn : Aleksander Nevski Cathedral  ...                      [Aleksander Nevski Cathedral]\n",
              "4                   Reval : Strandpforten installation  ...                                                 []\n",
              "..                                                 ...  ...                                                ...\n",
              "111  Talvinen näkymä Antinkadulta (=Lönnrotinkatu),...  ...         [Antinkadulta, Vanhan kirkon, Kirkkotoria]\n",
              "112       Eerikinkatu 6, 4, 2. Taustalla Yrjönkatu 25.  ...                           [Eerikinkatu, Yrjönkatu]\n",
              "113  Vappukulkue ylittämässä Pitkääsiltaa matkalla ...  ...                       [Pitkääsiltaa, Stadionille.]\n",
              "114  Pallopeliä pelataan Kalliolinnassa. Kalliolinn...  ...                  [Kalliolinnassa. Kalliolinnantie]\n",
              "115  Autojen pysäköintoalue Kampin kolmiossa, Runeb...  ...  [Kampin, Runeberginkadun, Malminkadun, Malmink...\n",
              "\n",
              "[116 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    }
  ]
}